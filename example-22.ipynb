{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Encoder Output Shape: (32, 45, 64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filehbgjcdvx.py\", line 12, in tf__call\n        x_transformed = ag__.converted_call(ag__.ld(self).transformer, (ag__.ld(x_encoded),), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'multi_scale_attention_transformer_1' (type MultiScaleAttentionTransformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_11879/3514068495.py\", line 70, in call  *\n            x_transformed = self.transformer(x_encoded)\n        File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 158, in error_handler\n            del bound_signature\n    \n        TypeError: Exception encountered when calling layer 'sequential_2' (type Sequential).\n        \n        MultiHeadAttention.call() missing 1 required positional argument: 'value'\n        \n        Call arguments received by layer 'sequential_2' (type Sequential):\n          • inputs=tf.Tensor(shape=(32, 45, 64), dtype=float32)\n          • training=True\n          • mask=None\n    \n    \n    Call arguments received by layer 'multi_scale_attention_transformer_1' (type MultiScaleAttentionTransformer):\n      • x=tf.Tensor(shape=(32, 45, 5), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m tensorboard_callback \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mTensorBoard(log_dir\u001b[39m=\u001b[39mlogdir)\n\u001b[1;32m    110\u001b[0m \u001b[39m# Train the model with the TensorBoard callback and validation data\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    112\u001b[0m     x_train, y_train,\n\u001b[1;32m    113\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m    114\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m    115\u001b[0m     validation_data\u001b[39m=\u001b[39m(x_val, y_val),  \u001b[39m# Add validation data here\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     callbacks\u001b[39m=\u001b[39m[tensorboard_callback]\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[39m# Use the trained model for weather forecasting\u001b[39;00m\n\u001b[1;32m    120\u001b[0m forecast \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_train[:\u001b[39m1\u001b[39m])  \u001b[39m# Make a forecast for the first sequence\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekhz3c1y6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filehbgjcdvx.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m x_encoded \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mencoder, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEncoder Output Shape: \u001b[39m\u001b[39m{\u001b[39;00mag__\u001b[39m.\u001b[39mld(x_encoded)\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m x_transformed \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtransformer, (ag__\u001b[39m.\u001b[39mld(x_encoded),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMulti-Head Attention Output Shape: \u001b[39m\u001b[39m{\u001b[39;00mag__\u001b[39m.\u001b[39mld(x_transformed)\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m x_decoded \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdecoder, (ag__\u001b[39m.\u001b[39mld(x_transformed),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filehbgjcdvx.py\", line 12, in tf__call\n        x_transformed = ag__.converted_call(ag__.ld(self).transformer, (ag__.ld(x_encoded),), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'multi_scale_attention_transformer_1' (type MultiScaleAttentionTransformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_11879/3514068495.py\", line 70, in call  *\n            x_transformed = self.transformer(x_encoded)\n        File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/xianjia/Workspace/miniconda3/envs/uwb-lstm-env/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 158, in error_handler\n            del bound_signature\n    \n        TypeError: Exception encountered when calling layer 'sequential_2' (type Sequential).\n        \n        MultiHeadAttention.call() missing 1 required positional argument: 'value'\n        \n        Call arguments received by layer 'sequential_2' (type Sequential):\n          • inputs=tf.Tensor(shape=(32, 45, 64), dtype=float32)\n          • training=True\n          • mask=None\n    \n    \n    Call arguments received by layer 'multi_scale_attention_transformer_1' (type MultiScaleAttentionTransformer):\n      • x=tf.Tensor(shape=(32, 45, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "# Simulated weather data (replace with your own dataset)\n",
    "num_samples = 1000\n",
    "sequence_length = 46\n",
    "num_features = 5\n",
    "\n",
    "# Generate synthetic weather data (replace with actual data loading)\n",
    "weather_data = np.random.rand(num_samples, sequence_length, num_features)\n",
    "\n",
    "# Split your dataset into training and validation sets\n",
    "# Assuming you have a dataset named 'weather_data'\n",
    "from sklearn import model_selection\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "    weather_data[:, :-1, :],  # Input: all time steps except the last\n",
    "    weather_data[:, -1:, :],   # Target: all time steps except the first\n",
    "    test_size=0.2,  # Adjust the validation split as needed\n",
    "    random_state=42  # Set a random seed for reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "class MultiScaleAttentionTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_heads, d_model, d_ff, sequence_length, num_features):\n",
    "        super(MultiScaleAttentionTransformer, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            keras.layers.Input(shape=(None, num_features)),  # Allow dynamic batch size\n",
    "            keras.layers.Conv1D(d_model, kernel_size=1, activation='relu'),\n",
    "            keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "\n",
    "        self.transformer = keras.Sequential([\n",
    "            keras.layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, \n",
    "                key_dim=d_model // num_heads,  # Adjust key dimension\n",
    "                value_dim=num_features  # Specify value dimension\n",
    "            ),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.LayerNormalization(epsilon=1e-6),\n",
    "            keras.layers.Conv1D(d_ff, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.decoder = keras.Sequential([\n",
    "            keras.layers.Conv1D(num_features, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "        self.downsampler = keras.Sequential([\n",
    "            keras.layers.Conv1D(d_model, kernel_size=3, strides=2, padding='same')\n",
    "        ])\n",
    "\n",
    "        self.upsampler = keras.Sequential([\n",
    "            keras.layers.Conv1D(d_model, kernel_size=3, padding='same')\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Encoder\n",
    "        x_encoded = self.encoder(x)\n",
    "        print(f\"Encoder Output Shape: {x_encoded.shape}\")\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        x_transformed = self.transformer(x_encoded)\n",
    "        print(f\"Multi-Head Attention Output Shape: {x_transformed.shape}\")\n",
    "        \n",
    "        # Decoder\n",
    "        x_decoded = self.decoder(x_transformed)\n",
    "        print(f\"Decoder Output Shape: {x_decoded.shape}\")\n",
    "        \n",
    "        # Multi-Scale Attention\n",
    "        x_downsampled = self.downsampler(x_encoded)\n",
    "        print(f\"Downsampled Output Shape: {x_downsampled.shape}\")\n",
    "        x_upsampled = tf.image.resize(x_transformed, (self.sequence_length, self.num_features))\n",
    "        print(f\"Upsampled Output Shape: {x_upsampled.shape}\")\n",
    "        x_multi_scale_attention = x_decoded + x_downsampled + x_upsampled\n",
    "        print(f\"Multi-Scale Attention Output Shape: {x_multi_scale_attention.shape}\")\n",
    "        \n",
    "        return x_multi_scale_attention\n",
    "\n",
    "# Hyperparameters\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "d_ff = 128\n",
    "\n",
    "# Create the model\n",
    "model = MultiScaleAttentionTransformer(num_heads, d_model, d_ff, sequence_length, num_features)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Prepare the training data\n",
    "# x_train = weather_data[:, :-1, :]  # Input: all time steps except the last\n",
    "# y_train = weather_data[:, 1:, :]   # Target: all time steps except the first\n",
    "\n",
    "# Convert the training data to TensorFlow tensors\n",
    "# x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "# y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "\n",
    "# Set up TensorBoard callback with log directory\n",
    "logdir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model with the TensorBoard callback and validation data\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),  # Add validation data here\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Use the trained model for weather forecasting\n",
    "forecast = model.predict(x_train[:1])  # Make a forecast for the first sequence\n",
    "\n",
    "print(\"Forecasted Weather:\")\n",
    "print(forecast)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
