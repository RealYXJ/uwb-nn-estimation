{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 13:40:33.160489: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33393/2704476149.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train, y_train = np.array(X_train), np.array(y_train)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sequence_length = 30\n",
    "n_steps = sequence_length\n",
    "input_dim = 3  \n",
    "\n",
    "\n",
    "# # Generate random synthetic data\n",
    "# synthetic_data = np.random.rand(num_samples, sequence_length, input_dim)\n",
    "\n",
    "# # Split the data into inputs and outputs\n",
    "# X = synthetic_data[:, :-1]  # Input data (sequence_length - 1 time steps)\n",
    "# y = synthetic_data[:, -1]   # Output data (1 time step ahead)\n",
    "\n",
    "dataset = pd.read_csv(\"data/train_newdata.csv\", delimiter=',')\n",
    "# print(dataset.head)\n",
    "\n",
    "dataset_selected = dataset[[\"node1\", \"node2\", \"uwb_range\", \"tb_node1_yaw\", \"tb_node2_yaw\",  \"error\"]] #\"mocap_range\",\n",
    "\n",
    "uwb_pair = [(1,4), (2,4), (3,4),(4,5),(1,2), (1,3), (1,5), (2,3), (2,4), (2,5), (3,5)]\n",
    "uwb_data = []\n",
    "for up in uwb_pair:\n",
    "    rslt_df = dataset_selected.loc[dataset_selected['node1'] == up[0]]\n",
    "    rslt_df_new = rslt_df.loc[rslt_df['node2'] == up[1]]\n",
    "    uwb_data.append(rslt_df_new[[\"uwb_range\", \"tb_node1_yaw\", \"tb_node2_yaw\",  \"error\"]].iloc[:,:].values)\n",
    "\n",
    "print(uwb_data[1].shape)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for inx, dataset_train in enumerate(uwb_data):\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    for i in range(n_steps,dataset_train.shape[0]):\n",
    "        x_temp.append(dataset_train[i-n_steps:i,0:3])\n",
    "        y_temp.append(dataset_train[i,3:])\n",
    "    # print(np.array(x_temp).shape, np.array(y_temp).shape)\n",
    "    X_train.append(np.array(x_temp))\n",
    "    y_train.append(np.array(y_temp))\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Multi-Scale Multi-Head Attention model\n",
    "class MultiScaleMultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, num_scales, name=\"multi_scale_multi_head_attention\"):\n",
    "        super(MultiScaleMultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_scales = num_scales\n",
    "        self.head_size = d_model // num_heads\n",
    "\n",
    "        assert self.head_size * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Define multi-head attention layers for each scale\n",
    "        self.attention_layers = [tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=self.head_size,\n",
    "            value_dim=self.head_size,\n",
    "        ) for _ in range(num_scales)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply multi-head attention for each scale\n",
    "        attention_outputs = []\n",
    "        for i in range(self.num_scales):\n",
    "            attention_output = self.attention_layers[i](inputs, inputs)\n",
    "            attention_outputs.append(attention_output)\n",
    "\n",
    "        # Concatenate attention outputs from different scales\n",
    "        concatenated_attention = tf.concat(attention_outputs, axis=-1)\n",
    "\n",
    "        return concatenated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_models = []\n",
    "for inx in range(X_train.shape[0]):\n",
    "    # Create a simple forecasting model using Multi-Scale Multi-Head Attention\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(sequence_length, input_dim)),\n",
    "        MultiScaleMultiHeadAttention(num_heads=4, d_model=32, num_scales=3),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1)  # Output layer for forecasting\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    lstm_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list = []\n",
    "earlyStopCallBack = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
    "for inx, lm in enumerate(lstm_models):\n",
    "    print(f\"------------ In data pair {inx} --------------\")\n",
    "    history = lm.fit(X_train[inx], y_train[inx], epochs=5, batch_size=16, validation_split=0.3, callbacks=[earlyStopCallBack] )\n",
    "    history_list.append(history)\n",
    "    \n",
    "print(f\"------------ Training Ended --------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in history_list:\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model mean_squared_error')\n",
    "    plt.ylabel('mean_squared_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/test_newdata.csv\", delimiter=',')\n",
    "# print(dataset.head)\n",
    "\n",
    "uwb_pair = [(1,4), (2,4), (3,4),(4,5),(1,2), (1,3), (1,5), (2,3), (2,4), (2,5), (3,5)]\n",
    "uwb_data_test = []\n",
    "for up in uwb_pair:\n",
    "    rslt_df = dataset.loc[dataset['node1'] == up[0]]\n",
    "    rslt_df_new = rslt_df.loc[rslt_df['node2'] == up[1]]\n",
    "    uwb_data_test.append(rslt_df_new[[\"uwb_range\", \"tb_node1_yaw\", \"tb_node2_yaw\",  \"error\"]].iloc[:200,:].values)\n",
    "\n",
    "print(uwb_data_test[1].shape)\n",
    "\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for inx, dataset_test in enumerate(uwb_data_test):\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    for i in range(n_steps,dataset_test.shape[0]):\n",
    "        x_temp.append(dataset_test[i-n_steps:i,0:3])\n",
    "        y_temp.append(dataset_test[i,3:])\n",
    "    print(np.array(x_temp).shape, np.array(y_temp).shape)\n",
    "    X_test.append(np.array(x_temp))\n",
    "    y_test.append(np.array(y_temp))\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "print(X_test[1].shape, y_test[1].shape)\n",
    "\n",
    "Predicted_X = []\n",
    "for inx in range(len(lstm_models)):\n",
    "    predict_x = lstm_models[inx].predict(X_test[inx], verbose=0)\n",
    "    Predicted_X.append(predict_x)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    print(mse(y_test[inx], predict_x).numpy())\n",
    "\n",
    "\n",
    "for inx in range(len(lstm_models)):\n",
    "    lstm_models[inx].save('models/lstm_uwb_{}'.format(inx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(len(Predicted_X)):\n",
    "    plt.plot(y_test[p], color='red', label= 'Real UWB Error')\n",
    "    plt.plot(Predicted_X[p], color='blue', label='Estimated UWB Error')\n",
    "    plt.title(\"UWB Error Estimation\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('UWB Error')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uwb-lstm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
