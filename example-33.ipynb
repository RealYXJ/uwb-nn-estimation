{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 11:33:25.429981: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 11:33:26.577079: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 25ms/step - loss: 0.1032\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.0852\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.0846\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.0842\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0843\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.0838\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.0837\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.0841\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0843\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0839\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "Weather Forecast: [[0.47006938]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic weather data\n",
    "num_samples = 1000\n",
    "sequence_length = 10\n",
    "input_dim = 5  # Temperature, humidity, wind speed, etc.\n",
    "\n",
    "# Generate random synthetic data\n",
    "synthetic_data = np.random.rand(num_samples, sequence_length, input_dim)\n",
    "\n",
    "# Split the data into inputs and outputs\n",
    "X = synthetic_data[:, :-1]  # Input data (sequence_length - 1 time steps)\n",
    "y = synthetic_data[:, -1]   # Output data (1 time step ahead)\n",
    "\n",
    "# Define the Multi-Scale Multi-Head Attention model\n",
    "class MultiScaleMultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, num_scales, name=\"multi_scale_multi_head_attention\"):\n",
    "        super(MultiScaleMultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_scales = num_scales\n",
    "        self.head_size = d_model // num_heads\n",
    "\n",
    "        assert self.head_size * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Define multi-head attention layers for each scale\n",
    "        self.attention_layers = [tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=self.head_size,\n",
    "            value_dim=self.head_size,\n",
    "        ) for _ in range(num_scales)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply multi-head attention for each scale\n",
    "        attention_outputs = []\n",
    "        for i in range(self.num_scales):\n",
    "            attention_output = self.attention_layers[i](inputs, inputs)\n",
    "            attention_outputs.append(attention_output)\n",
    "\n",
    "        # Concatenate attention outputs from different scales\n",
    "        concatenated_attention = tf.concat(attention_outputs, axis=-1)\n",
    "\n",
    "        return concatenated_attention\n",
    "\n",
    "# Create a simple forecasting model using Multi-Scale Multi-Head Attention\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(sequence_length - 1, input_dim)),\n",
    "    MultiScaleMultiHeadAttention(num_heads=4, d_model=32, num_scales=3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for forecasting\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform weather forecasting\n",
    "new_input_data = np.random.rand(1, sequence_length - 1, input_dim)  # Replace with real input data\n",
    "forecast = model.predict(new_input_data)\n",
    "\n",
    "print(\"Weather Forecast:\", forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uwb-lstm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
